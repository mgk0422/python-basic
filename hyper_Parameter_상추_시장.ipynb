{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyper_Parameter_상추_시장",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNRQcCCbezFModKpcxJEWFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgk0422/python-basic/blob/master/hyper_Parameter_%EC%83%81%EC%B6%94_%EC%8B%9C%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ght0hmsAgAED"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kppA_WKAr_Tm"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwuipB0vIkg1"
      },
      "source": [
        "상추"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V3Mgndss9iT"
      },
      "source": [
        "dataset = pd.read_excel('/content/상추_시장.xlsx')\n",
        "# dataset = pd.merge(dataset, wholesale, how='left', on='일자' )\n",
        "# dataset.head()\n",
        "# dataset = pd.read_excel('/content/무_특_반입_경락_시장_날씨_Holy_유가.xlsx')\n",
        "# dataset = dataset.drop(['일자','품목명','품목코드','유무','평균 5cm 지중온도(°C)','평균 지면온도(°C)','최고기온(°C)','합계 일조시간(hr)','최저기온(°C)'], axis=1)\n",
        "# dataset = dataset.drop(['일자', '최대 풍속(m/s)',\t'평균 풍속(m/s)',\t'평균 상대습도(%)',\t'합계 일조시간(hr)',\t'평균 지면온도(°C)',\t'평균 5cm 지중온도(°C)', '여부'], axis=1)\n",
        "# dataset = dataset[['도매가격', '100g가격', '유가 전국평균가격', '평균 풍속(m/s)', '여부', '최고기온(°C)', '평균 지면온도(°C)', '평균 5cm 지중온도(°C)', '일강수량(mm)', '반입량']]\n",
        "# dataset = dataset.drop(['일자','Unnamed: 0'], axis=1)\n",
        "#dataset = dataset.drop(['일자'], axis=1)\n",
        "dataset = dataset.dropna()\n",
        "dataset.rename(columns={'경락가평균가겨':'경락가평균가격'},inplace=True)\n",
        "dataset.head()\n",
        "# dataset.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNdMa9bJ0Zf8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw4AABkdK8qO"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ljCFN_YQdpU"
      },
      "source": [
        "상추_더미 = pd.read_excel('/content/상추_더미데이터.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCsZ-YzIQkw2"
      },
      "source": [
        "상추_더미.drop(['일자 ','가격'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBxwaUD8QqHF"
      },
      "source": [
        "상추_더미.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF-AMQvZ0bb5"
      },
      "source": [
        "dataset2=dataset[['가격','경락가평균가격','반입량','유가 전국평균가격','도매가격']]\r\n",
        "dataset2.head() #위 변수들만 dataset2로 지정 log 변환해줌\r\n",
        "\r\n",
        "dataset2['가격']= np.log1p(dataset2['가격'])\r\n",
        "dataset2['경락가평균가격']= np.log1p(dataset2['경락가평균가격'])\r\n",
        "dataset2['반입량']= np.log1p(dataset2['반입량'])\r\n",
        "dataset2['유가 전국평균가격']= np.log1p(dataset2['유가 전국평균가격'])\r\n",
        "dataset2['도매가격']= np.log1p(dataset2['도매가격'])\r\n",
        "\r\n",
        "dataset1=dataset[['유무','최저기온(°C)','최고기온(°C)','일강수량(mm)']]\r\n",
        "\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scalar=MinMaxScaler()\r\n",
        "dataset1[:]=scalar.fit_transform(dataset1[:]) #전부다 MinMax로 \r\n",
        "\r\n",
        "dataset=pd.concat([dataset1,dataset2],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCvv_BRjtGan"
      },
      "source": [
        "y_target = dataset['가격']\n",
        "X_data = dataset.drop(['가격'], axis=1, inplace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyeZE--HtHWl"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.15, random_state=140)\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh77Y8bXtNQW"
      },
      "source": [
        "# 검증해주는 함수\n",
        "def get_model_cv_prediction(model, X_data, y_target):\n",
        "  neg_mse_scores=cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 3)\n",
        "  rmse_scores = np.sqrt(-1 * neg_mse_scores)\n",
        "  avg_rmse = np.mean(rmse_scores)\n",
        "  model.fit(X_data, y_target)\n",
        "  score = model.score(X_data, y_target)\n",
        "  kf_cv_scores = cross_val_score(model, X_data, y_target)\n",
        "  print('#### ', model.__class__.__name__, ' ####')\n",
        "  print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))\n",
        "  print('score : ', score)\n",
        "  print(\"K-fold CV scores : {}\" .format(kf_cv_scores))\n",
        "  print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ital2WhZgAh0"
      },
      "source": [
        "## XGBoost Regressor 하이퍼파라미터\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql8ToBz_gP3i"
      },
      "source": [
        "# XGBoost Hypter Parameter Tuning\n",
        "def XGBhyperParameterTuning(X_train, y_train):\n",
        "    param_tuning = {\n",
        "        'learning_rate': [0.01, 0.03, 0.1],\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'min_child_weight': [1, 3, 5],\n",
        "        'subsample': [0.5, 0.7],\n",
        "        'colsample_bytree': [0.5, 0.7],\n",
        "        'n_estimators' : [100, 200, 500],\n",
        "        'objective': ['reg:squarederror'],\n",
        "        'gamma' : [0.1, 0.2, 0.3, 0.4],\n",
        "\n",
        "    }\n",
        "\n",
        "    xgb_model = XGBRegressor()\n",
        "\n",
        "    gsearch = GridSearchCV(estimator = xgb_model,\n",
        "                           param_grid = param_tuning,                        \n",
        "                           #scoring = 'neg_mean_absolute_error', #MAE\n",
        "                           #scoring = 'neg_mean_squared_error',  #MSE\n",
        "                           cv = 3,\n",
        "                           n_jobs = -1,\n",
        "                           verbose = 1)\n",
        "\n",
        "    gsearch.fit(X_train,y_train)\n",
        "\n",
        "    return gsearch.best_params_\n",
        "\n",
        "def RFRhyperParameterTuning(X_train, y_train):\n",
        "    param_tuning = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'max_features': [2, 3],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [8, 10, 12],\n",
        "    'n_estimators': [100, 200, 300, 1000],\n",
        "    \n",
        "    }\n",
        "\n",
        "    rf_model = RandomForestRegressor()\n",
        "\n",
        "    gsearch = GridSearchCV(estimator = rf_model,\n",
        "                           param_grid = param_tuning,                        \n",
        "                           #scoring = 'neg_mean_absolute_error', #MAE\n",
        "                           #scoring = 'neg_mean_squared_error',  #MSE\n",
        "                           cv = 3,\n",
        "                           n_jobs = -1,\n",
        "                           verbose = 1)\n",
        "\n",
        "    gsearch.fit(X_train,y_train)\n",
        "\n",
        "    return gsearch.best_params_\n",
        "\n",
        "def LGBMhyperParameterTuning(X_train, y_train):\n",
        "    param_tuning = {\n",
        "        'learning_rate': [0.01, 0.03, 0.1],\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'min_child_samples' : [10, 20, 30],\n",
        "        'subsample' : [0.5, 0.7],\n",
        "        'colsample_bytree': [0.5, 0.7],\n",
        "        'n_estimators' : [100, 200, 500],\n",
        "        'objective': ['regression'],\n",
        "        'gamma' : [0.1, 0.2, 0.3],\n",
        "        'num_leaves' : [6, 8, 10]\n",
        "\n",
        "    }\n",
        "\n",
        "    lgbm = LGBMRegressor()\n",
        "\n",
        "    gsearch = GridSearchCV(estimator = lgbm,\n",
        "                           param_grid = param_tuning,                        \n",
        "                           #scoring = 'neg_mean_absolute_error', #MAE\n",
        "                           #scoring = 'neg_mean_squared_error',  #MSE\n",
        "                           cv = 3,\n",
        "                           n_jobs = -1,\n",
        "                           verbose = 1)\n",
        "\n",
        "    gsearch.fit(X_train,y_train)\n",
        "\n",
        "    return gsearch.best_params_\n",
        "\n",
        "def GBRhyperParameterTuning(X_train, y_train):\n",
        "    \n",
        "    param_tuning = {\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.5, 0.7],\n",
        "        'n_estimators' : [100, 200, 500]\n",
        "\n",
        "    }\n",
        "\n",
        "    gb_model = GradientBoostingRegressor()\n",
        "\n",
        "    gsearch = GridSearchCV(estimator = gb_model,\n",
        "                           param_grid = param_tuning,                        \n",
        "                           #scoring = 'neg_mean_absolute_error', #MAE\n",
        "                           #scoring = 'neg_mean_squared_error',  #MSE\n",
        "                           cv = 3,\n",
        "                           n_jobs = -1,\n",
        "                           verbose = 1)\n",
        "\n",
        "    gsearch.fit(X_train,y_train)\n",
        "\n",
        "    return gsearch.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S11ES3a40jX"
      },
      "source": [
        "xgb_parameter = XGBhyperParameterTuning(X_train, y_train)\n",
        "xgb_parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S0k-S5B40sa"
      },
      "source": [
        "rf_parameter = RFRhyperParameterTuning(X_train, y_train)\n",
        "rf_parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN4Gz1kf40y9"
      },
      "source": [
        "lgbm_parameter = LGBMhyperParameterTuning(X_train, y_train)\n",
        "lgbm_parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0yX6PRhq8Vg"
      },
      "source": [
        "\n",
        "#rf_parameter = RFRhyperParameterTuning(X_train, y_train)\n",
        "#rf_parameter\n",
        "\n",
        "gbr_parameter= GBRhyperParameterTuning(X_train, y_train)\n",
        "gbr_parameter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTFOYkPFuH-n"
      },
      "source": [
        "# 모델 생성 및 예측\n",
        "xgb_reg = XGBRegressor(colsample_bytree = 0.7, \n",
        "                       gamma = 0.2,\n",
        "                       learning_rate = 0.1,\n",
        "                       max_depth = 7,\n",
        "                       min_child_weight = 5,\n",
        "                       n_estimators = 2100,\n",
        "                       objective = 'reg:squarederror',\n",
        "                       subsample = 0.7)\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=0, \n",
        "                               bootstrap = [True],\n",
        "                               max_depth = 10,\n",
        "                               max_features = 3,\n",
        "                               min_samples_leaf = 3,\n",
        "                               min_samples_split = 10,\n",
        "                               n_estimators = 300\n",
        "                               )\n",
        "\n",
        "lgb_reg = LGBMRegressor(colsample_bytree = 0.7,\n",
        "                        gamma = 0.1,\n",
        "                        learning_rate = 0.01,\n",
        "                        max_depth = 3,\n",
        "                        min_child_samples = 10,\n",
        "                        n_estimators = 500,\n",
        "                        num_leaves =8,\n",
        "                        objective = 'regression',\n",
        "                        subsample = 0.5)\n",
        "\n",
        "gb_reg = GradientBoostingRegressor(random_state=0, \n",
        "                                   learning_rate=0.01,\n",
        "                                   max_depth=3,\n",
        "                                   n_estimators=500,\n",
        "                                   subsample=0.5\n",
        "                                   )\n",
        "\n",
        "models = [lgb_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]\n",
        "\n",
        "for model in models:\n",
        "  get_model_cv_prediction(model, X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2GAHHKpuIA-"
      },
      "source": [
        "ypred = rf_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, ypred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTja4A6yuIDD"
      },
      "source": [
        "x_ax = range(len(y_test))\n",
        "plt.plot(x_ax, y_test, label=\"Original Price\")\n",
        "plt.plot(x_ax, ypred, label=\"Predicted Price\")\n",
        "plt.title(\"Price Prediction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLwaHJIyuIE9"
      },
      "source": [
        "from sklearn.ensemble import VotingRegressor\r\n",
        "\r\n",
        "er = VotingRegressor([('xgb_reg', xgb_reg), ('gb_reg', gb_reg),('rf_reg',rf_reg),('lgb_reg',lgb_reg)])\r\n",
        "print(er.fit(X_train, y_train).predict(X_test))\r\n",
        "y_pred = er.fit(X_train, y_train).predict(X_test)\r\n",
        "er.fit(X_train, y_train).score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaxrslxBuRcV"
      },
      "source": [
        "x_ax = range(len(y_test))\r\n",
        "plt.plot(x_ax, y_test, label=\"Original Price\")\r\n",
        "plt.plot(x_ax, y_pred, label=\"Predicted Price\")\r\n",
        "plt.title(\"Price Prediction\")\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtpJ8UqyuRej"
      },
      "source": [
        "from sklearn.datasets import load_diabetes\r\n",
        "from sklearn.linear_model import RidgeCV\r\n",
        "from sklearn.svm import LinearSVR\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.ensemble import StackingRegressor\r\n",
        "y_target = dataset['가격']\r\n",
        "X_data = dataset.drop(['가격'], axis=1, inplace=False)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.15, random_state=140)\r\n",
        "estimators= ([('xgb_reg', xgb_reg), ('rf_reg', rf_reg), ('lgb_reg', lgb_reg), ('gb_reg', gb_reg)])\r\n",
        "reg = StackingRegressor(estimators=estimators,final_estimator=RandomForestRegressor(n_estimators=10,random_state=42))\r\n",
        "reg.fit(X_train, y_train).score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkoskiPhuRg0"
      },
      "source": [
        "from sklearn.ensemble import VotingRegressor\r\n",
        "\r\n",
        "er = VotingRegressor([('xgb_reg', xgb_reg), ('gb_reg', gb_reg),('rf_reg',rf_reg),('lgb_reg',lgb_reg)])\r\n",
        "y_pred = er.fit(X_train, y_train).predict(상추_더미)\r\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usxyZ8Z6Q5ik"
      },
      "source": [
        "상추_더미['예측가격']=y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdipNJHkRm-O"
      },
      "source": [
        "상추_더미.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNSp_2uPRqTu"
      },
      "source": [
        "상추_더미.to_excel('상추_더미(예측).xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUESrSpeRxH7"
      },
      "source": [
        "abc = []\r\n",
        "for alg in er.named_estimators:\r\n",
        "    clf = er.named_estimators[alg]\r\n",
        "    a = clf.__class__.__name__\r\n",
        "    b = [pd.DataFrame(sorted(zip(clf.feature_importances_,X_train.columns)), columns=['Value','Feature'])]\r\n",
        "    abc.append({a:b})\r\n",
        "\r\n",
        "abc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q4hWpDmG2lz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}