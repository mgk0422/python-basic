{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextAnalytics",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPIXNbcQ15rh5bQGKnEhhGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgk0422/python-basic/blob/master/TextAnalytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOc1vpz6Go5i"
      },
      "source": [
        "# 텍스트 사전 준비작업(텍스트 전처리) : 텍스트를 피처로 만들기 위해 미리 클렌징, 대/소문자 변경, 특수문자 삭제 등 클렌징 작업, 단어 등의 토큰화 작업, 의미없는 (Stop word) 제거작업, 어근 추출등 텍스트 정규화\r\n",
        "# 피처 벡터화 : 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터 값을 할당\r\n",
        "# ML 모델 수립 및 학습/예측/평가 : 피처 벡터화된 데이터 세트에 ML모델을 적용해 학습/예측/평가를 수행\r\n",
        "\r\n",
        "# 텍스트 정규화 작업 (클렌징, 토큰화.필터링/스톱 워드 제거/철자 수정,Stemming, Lemmatization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz0EOY_YWPBz"
      },
      "source": [
        "from nltk import word_tokenize\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "# 단어 토큰화, 단어의 순서가 중요하지 않은 경우 문장 토큰화를 사용하지않고 단어 토큰화만 사용해도 충분\r\n",
        "sentence=\"The Matrix is everywhere its all around us, here even in this room\"\r\n",
        "words=word_tokenize(sentence)\r\n",
        "print(type(words),len(words))\r\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlWn57-9Wpvg"
      },
      "source": [
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\r\n",
        "               You can see it out your window or on your television. \\\r\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\r\n",
        "\r\n",
        "from nltk import word_tokenize,sent_tokenize\r\n",
        "\r\n",
        "# 여러개 문장으로 된 입력데이터를 문장별로 단어 토큰화\r\n",
        "def tokenize_text(text):\r\n",
        "\r\n",
        "    #문장별로 분리토큰\r\n",
        "    sentences=sent_tokenize(text)\r\n",
        "\r\n",
        "    # 분리된 문장별 단어 토큰화\r\n",
        "    word_tokens=[word_tokenize(sentence) for sentence in sentences]\r\n",
        "    return word_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHJ_y-LtY0u8"
      },
      "source": [
        "# 여러 문장에 대해 문장별 단어 토큰화 수행\r\n",
        "word_tokens = tokenize_text(text_sample)\r\n",
        "print(type(word_tokens),len(word_tokens))\r\n",
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m15yv2wZGb4"
      },
      "source": [
        "# 스톱 워드 제거 : 분석에 큰 의미 없는 단어 지정 / is, the, a, will 등\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "print(\"영어 stop words 개수:\",len(nltk.corpus.stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ecRItz9ak3S"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "stopwords=nltk.corpus.stopwords.words('english')\r\n",
        "all_tokens=[]\r\n",
        "for sentence in word_tokens:\r\n",
        "    filtered_words=[]\r\n",
        "    #개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\r\n",
        "    for word in sentence:\r\n",
        "        word=word.lower() #소문자로 변환\r\n",
        "        # x토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\r\n",
        "        if word not in stopwords:\r\n",
        "            filtered_words.append(word)\r\n",
        "    all_tokens.append(filtered_words)\r\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVqVESJSbd2l"
      },
      "source": [
        "# stemming 과 Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는것\r\n",
        "# Bag of Words : Bow : 문서가 가지는 모든단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여해 피처값을 추출하는 모델\r\n",
        "# 피처 벡터화 : 텍스트는 특정 의미를 가지는 숫자형 값인 벡터값으로 변환 . 각 문서의 텍스트를 단어로 추출해 피처로 할당하고\r\n",
        "# 각 단어의 발생 빈도와 같은 값을 피처에 값으로 부여해 각 문서를 이 단어 피처의 발생 빈도 값으로 구성된 벡터로 만드는 기법\r\n",
        "# 사이킷런 CountVectorizer 클래스 : 카운트 기반의 벡터화를 구현한 클래스\r\n",
        "# 대규모 행렬의 대부분의 값을 0이 차지하는 행렬을 가르켜 희소행렬이라고 한다.(BOW 형태를 가진 언어 모델의 피처 벡터화는 대부분 희소 행렬)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjXUfLQzhrfU"
      },
      "source": [
        "# 희소 행렬 -COO형식 : 0이 아닌 데이터만 별도의 데이터 배열에 저장하고, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "dense=np.array([[3,0,1],[0,2,0]])\r\n",
        "\r\n",
        "from scipy import sparse\r\n",
        "# 0이 아닌 데이터 추출\r\n",
        "data=np.array([3,1,2])\r\n",
        "\r\n",
        "from scipy import sparse\r\n",
        "\r\n",
        "# 0 이 아닌 데이터 추출\r\n",
        "data = np.array([3,1,2])\r\n",
        "\r\n",
        "# 행 위치와 열 위치를 각각 array로 생성 \r\n",
        "row_pos = np.array([0,0,1])\r\n",
        "col_pos = np.array([0,2,1])\r\n",
        "\r\n",
        "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\r\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))\r\n",
        "\r\n",
        "sparse_coo.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4hLxy9xiZhh"
      },
      "source": [
        "# 희소 행렬 - CSR 형식\r\n",
        "\r\n",
        "from scipy import sparse\r\n",
        "\r\n",
        "dense2 = np.array([[0,0,1,0,0,5],\r\n",
        "             [1,4,0,3,2,5],\r\n",
        "             [0,6,0,3,0,0],\r\n",
        "             [2,0,0,0,0,0],\r\n",
        "             [0,0,0,7,0,8],\r\n",
        "             [1,0,0,0,0,0]])\r\n",
        "\r\n",
        "# 0 이 아닌 데이터 추출\r\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\r\n",
        "\r\n",
        "# 행 위치와 열 위치를 각각 array로 생성 \r\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\r\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\r\n",
        "\r\n",
        "# COO 형식으로 변환 \r\n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\r\n",
        "\r\n",
        "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\r\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\r\n",
        "\r\n",
        "# CSR 형식으로 변환 \r\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\r\n",
        "\r\n",
        "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\r\n",
        "print(sparse_coo.toarray())\r\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\r\n",
        "print(sparse_csr.toarray())\r\n",
        "\r\n",
        "\r\n",
        "dense3 = np.array([[0,0,1,0,0,5],\r\n",
        "             [1,4,0,3,2,5],\r\n",
        "             [0,6,0,3,0,0],\r\n",
        "             [2,0,0,0,0,0],\r\n",
        "             [0,0,0,7,0,8],\r\n",
        "             [1,0,0,0,0,0]])\r\n",
        "\r\n",
        "coo = sparse.coo_matrix(dense3)\r\n",
        "csr = sparse.csr_matrix(dense3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgEh0ROfi3n-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}